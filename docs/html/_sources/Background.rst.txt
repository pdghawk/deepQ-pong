.. _Background:

Background
============

A simple google search of 'deep Q networks' will return a myriad of lengthy discussions
on deep Q networks, so we won't go into too much detail here.

For an informative set of lecture notes on the topic, see `these slides <http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf>`_.

The DeepMind paper on reinforcement learning for Atari games can be found `here <https://daiwk.github.io/assets/dqn.pdf>`_.


Let us highlight a few differences in the method from that of the deepmind paper,
assuming some knowledge of reinforcement learning.

Experience Replay
------------------

An important components of deep Q learning, is that the agent has a memory of what
actions it performed in different states of the game, and what state and reward
resulted from these actions. Let's denote a set of (state0,state1,action,reward)
as a transition. The agent is then trained on random batches of such transitions
from its memory.

In this code a slight variation on that is applied, where the batches are not a
random selection of previous transitions. Instead, the memory is split into three
parts - a winning move memory, a losing move memory, and a normal move memory. The
batches are created with a fraction of winning,losing, and normal moves. The fractions
of each type of move in the batch stays the same throughout the game. This is intended
to help the agent not 'forget' aspects of gameplay that it may see less regularly
as the game progresses. [Disclaimer: I'm not claiming to be the first person to do this,
it's entirely possible others have done this].

Double Deep Q Learning
------------------------

Since the Deepmind paper referenced above has been published, there have been a number
of improvements made and published in the literature. One such improvement is applying
double Q learning, to deep Q learning (called double-DQN), see `here <http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/doubledqn.pdf>`_.
Double-DQN provides more stable learning. The double-DQN methodology is applied in
deepQ-pong.

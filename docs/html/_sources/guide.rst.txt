User Guide
===========

This repo is designed for the training and testing of deep double-Q networks applied
to the Atari game 'Pong', using Tensorflow. For more information on the background
of the algorithm see :ref:`Background`. Here, will focus on how the code is arranged, and
how one can use it to solve Pong.

.. raw:: html

  <video controls src="_static/pong_game_epi200.mp4"></video>

This video shows how an agents plays after training for 200 episodes. The agent
is able to score points almost as well as the built-in pong player. Further training
would be expected to increase the agents ability, particularly with some hyperparameter
optimization.


Running The Code
------------------

In deepQ-pong/code one can find the package DQN, which contains the methods required
to solve Pong, and python scripts that will run the required methods within DQN.

The details of DQN, and its methods, are detailed in the documentation here: :ref:`DQN`

Here, we focus on the python scripts we can run in order to solve the problem, and
produce output.

The basic process is as follows:

1. Train an agent using a python script (single_train.py) that calls DQN.deepQ.train(), which also saves a checkpoint of the network at the end of training
2. Use another python script (play_from_ckpt.py) to load the checkpoint data, and play a game of pong using that agent, saving the resulting game as a numpy array
3. Use a final python script (video_from_array.py) to create an mp4 video file of a saved numpy array of a game.

Stages 2 and 3 can be ignored if no video output is desired

This structure allows one to train multiple agents, and create arrays of the agents
game on an AWS EC2 instance, and later download those arrays, and create a video
on the local machine. Note that if your local machine doesn't have a GPU the checkpoint
data maybe unable to load onto your local machine to play games, so stages 1 and 2
should both be performed an AWS GPU instance.

.. _AWS_TB:

Observing Results
------------------

During training, tensorboard summaries are produced. These can be observed during
and after training by calling tensorflow's tensorboard and observing in browser.

When running non-locally over ssh on an AWS instance, one can ssh with a link,
the -L option, in order that one can observe the tensorboard in the local browser:

~$ ssh -L 127.0.0.1:6006:127.0.0.1:6006 -i /path/to/key.pem ubuntuATaws_address

where the key.pem file and aws_address should be changed to your personal EC2 key
and address.

Note that when viewing scalars, the regular expression:

reward|step

is useful to view the average rewards and average steps per episode simultaneously


Using AWS EC2
--------------

Solving this problem requires a lot of computational power, GPUs are a fantastic
architecture for deep learning, happily we can access them for a small cost via
AWS EC2 instances.

There a several AWS helper functions provided in deepQ-pong, to make uploading
the code and downloading data to/from an AWS EC2 quick and easy.

More details about these helper functions can be found in :ref:`AWS_Scripts`

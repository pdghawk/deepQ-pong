
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Background &#8212; DeepQ-pong 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="User Guide" href="guide.html" />
    <link rel="prev" title="Read Me" href="README.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="background">
<span id="id1"></span><h1>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h1>
<p>A simple google search of ‘deep Q networks’ will return a myriad of lengthy discussions
on deep Q networks, so we won’t go into too much detail here.</p>
<p>For an informative set of lecture notes on the topic, see <a class="reference external" href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf">these slides</a>.</p>
<p>The DeepMind paper on reinforcement learning for Atari games can be found <a class="reference external" href="https://daiwk.github.io/assets/dqn.pdf">here</a>.</p>
<p>Let us highlight a few differences in the method from that of the deepmind paper,
assuming some knowledge of reinforcement learning.</p>
<div class="section" id="experience-replay">
<h2>Experience Replay<a class="headerlink" href="#experience-replay" title="Permalink to this headline">¶</a></h2>
<p>An important components of deep Q learning, is that the agent has a memory of what
actions it performed in different states of the game, and what state and reward
resulted from these actions. Let’s denote a set of (state0,state1,action,reward)
as a transition. The agent is then trained on random batches of such transitions
from its memory.</p>
<p>In this code a slight variation on that is applied, where the batches are not a
random selection of previous transitions. Instead, the memory is split into three
parts - a winning move memory, a losing move memory, and a normal move memory. The
batches are created with a fraction of winning,losing, and normal moves. The fractions
of each type of move in the batch stays the same throughout the game. This is intended
to help the agent not ‘forget’ aspects of gameplay that it may see less regularly
as the game progresses. [Disclaimer: I’m not claiming to be the first person to do this,
it’s entirely possible others have done this].</p>
</div>
<div class="section" id="double-deep-q-learning">
<h2>Double Deep Q Learning<a class="headerlink" href="#double-deep-q-learning" title="Permalink to this headline">¶</a></h2>
<p>Since the Deepmind paper referenced above has been published, there have been a number
of improvements made and published in the literature. One such improvement is applying
double Q learning, to deep Q learning (called double-DQN), see <a class="reference external" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/doubledqn.pdf">here</a>.
Double-DQN provides more stable learning. The double-DQN methodology is applied in
deepQ-pong.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">DeepQ-pong</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="README.html">Read Me</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Background</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#experience-replay">Experience Replay</a></li>
<li class="toctree-l2"><a class="reference internal" href="#double-deep-q-learning">Double Deep Q Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="DQN.html">DQN documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="aws.html">AWS Scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="License.html">License</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="README.html" title="previous chapter">Read Me</a></li>
      <li>Next: <a href="guide.html" title="next chapter">User Guide</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Peter Hawkins.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/Background.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>